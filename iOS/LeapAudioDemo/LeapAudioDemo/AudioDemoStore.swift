import AVFoundation
import Foundation
import LeapSDK
import Observation

struct AudioDemoMessage: Identifiable, Equatable {
  let id = UUID()
  let role: ChatMessageRole
  let text: String
  let audioData: Data?

  var isUser: Bool { role == .user }

  static func == (lhs: AudioDemoMessage, rhs: AudioDemoMessage) -> Bool {
    lhs.id == rhs.id && lhs.role == rhs.role && lhs.text == rhs.text
  }
}

@Observable
@MainActor
final class AudioDemoStore {
  var inputText: String = ""
  var messages: [AudioDemoMessage] = []
  var status: String?
  var streamingText: String = ""
  var isModelLoading = false
  var isGenerating = false
  var isRecording = false

  private let playbackManager = AudioPlaybackManager()
  private let recorder = AudioRecorder()
  private var conversation: Conversation?
  private var modelRunner: ModelRunner?
  private var streamingTask: Task<Void, Never>?

  init() {
    playbackManager.prepareSession()
  }

  func setupModel() async {
    guard modelRunner == nil else { return }
    isModelLoading = true
    status = "Loading model bundle..."

    guard let modelURL = findModelURL() else {
      status = "No GGUF model found in bundle. Add an audio-capable model first."
      isModelLoading = false
      return
    }

    do {
      let bundle = Bundle.main
      let mmProjPath = bundle.url(forResource: "mmproj-LFM2.5-Audio-1.5B-Q8_0", withExtension: "gguf")?.path()
      let audioTokenizerPath = bundle.url(forResource: "tokenizer-LFM2.5-Audio-1.5B-Q8_0", withExtension: "gguf")?.path()
      let vocoderPath = bundle.url(forResource: "vocoder-LFM2.5-Audio-1.5B-Q8_0", withExtension: "gguf")?.path()
      
      print("mmProjPath: \(mmProjPath ?? "nil")")
      print("audioTokenizerPath: \(audioTokenizerPath ?? "nil")")
      print("vocoderPath: \(vocoderPath ?? "nil")")
      
      var options = LiquidInferenceEngineOptions(
        bundlePath: modelURL.path(),
        contextSize: 8192,
        nGpuLayers: 0,
        mmProjPath: mmProjPath,
        audioDecoderPath: vocoderPath,
        audioTokenizerPath: audioTokenizerPath
      )
      let runner = try Leap.load(options: options)
      modelRunner = runner
      conversation = Conversation(
        modelRunner: runner,
        history: [
          ChatMessage(role: .system, content: [.text("Respond with interleaved text and audio.")])
        ])
      messages.append(
        AudioDemoMessage(
          role: .assistant,
          text: "Model loaded: \(modelURL.lastPathComponent)",
          audioData: nil
        )
      )
      status = "Ready"
    } catch {
      status = "Failed to load model: \(error.localizedDescription)"
    }

    isModelLoading = false
  }

  func sendTextPrompt() {
    let trimmed = inputText.trimmingCharacters(in: .whitespacesAndNewlines)
    guard !trimmed.isEmpty else { return }
    inputText = ""
    let message = ChatMessage(role: .user, content: [.text(trimmed)])
    appendUserMessage(text: trimmed, audioData: nil)
    streamResponse(for: message)
  }

  func toggleRecording() {
    if isRecording {
      recorder.stop()
      isRecording = false
      guard let capture = recorder.capture() else {
        status = "No audio captured."
        return
      }
      sendAudioPrompt(samples: capture.samples, sampleRate: capture.sampleRate)
    } else {
      do {
        try recorder.start()
        isRecording = true
        status = "Recording..."
      } catch {
        status = "Recording failed: \(error.localizedDescription)"
      }
    }
  }

  func cancelRecording() {
    recorder.cancel()
    isRecording = false
    status = "Recording cancelled."
  }

  func playAudio(_ data: Data) {
    playbackManager.play(wavData: data)
  }

  private func sendAudioPrompt(samples: [Float], sampleRate: Int) {
    guard !samples.isEmpty else {
      status = "Audio capture was empty."
      return
    }

    let content = ChatMessageContent.fromFloatSamples(samples, sampleRate: sampleRate)
    let chatMessage = ChatMessage(role: .user, content: [content])

    var display = "Audio prompt (\(samples.count) samples @ \(sampleRate) Hz)"
    if samples.count < sampleRate / 4 {
      display = "Audio prompt (~\(samples.count) samples)"
    }

    let audioData: Data?
    if case .audio(let data) = content {
      audioData = data
    } else {
      audioData = nil
    }

    appendUserMessage(text: display, audioData: audioData)
    streamResponse(for: chatMessage)
  }

  private func appendUserMessage(text: String, audioData: Data?) {
    messages.append(AudioDemoMessage(role: .user, text: text, audioData: audioData))
  }

  private func streamResponse(for message: ChatMessage) {
    guard let modelRunner else {
      status = "Model not ready yet."
      return
    }

    // LFM2.5 audio engine requires fresh conversation for each turn
    // ("messages are not replayable, need to start a new dialog")
    let freshConversation = Conversation(
      modelRunner: modelRunner,
      history: [
        ChatMessage(role: .system, content: [.text("Respond with interleaved text and audio.")])
      ])
    conversation = freshConversation

    playbackManager.reset()
    streamingTask?.cancel()
    streamingText = ""
    status = "Awaiting response..."
    isGenerating = true

    let stream = freshConversation.generateResponse(message: message)

    streamingTask = Task { [weak self] in
      guard let self else { return }
      do {
        for try await event in stream {
          if Task.isCancelled { break }
          await MainActor.run {
            self.handle(event)
          }
        }
      } catch {
        await MainActor.run {
          self.handleGenerationError(error)
        }
      }
      await MainActor.run {
        self.streamingTask = nil
      }
    }
  }

  private func handle(_ event: MessageResponse) {
    switch event {
    case .chunk(let text):
      streamingText.append(text)
    case .reasoningChunk:
      status = "Thinking..."
    case .audioSample(let samples, let sampleRate):
      playbackManager.enqueue(samples: samples, sampleRate: sampleRate)
      status = "Streaming audio..."
    case .functionCall(let calls):
      status = "Received function call: \(calls.count)"
    case .complete(let completion):
      finish(with: completion)
    }
  }

  private func finish(with completion: MessageCompletion) {
    print("=== Response Complete ===")
    print("Finish reason: \(completion.finishReason)")
    print("Content count: \(completion.message.content.count)")
    for (index, content) in completion.message.content.enumerated() {
      switch content {
      case .text(let t): print("  [\(index)] text: \(t.prefix(100))...")
      case .audio(let d): print("  [\(index)] audio: \(d.count) bytes")
      default: print("  [\(index)] other content type")
      }
    }
    
    let text = completion.message.content.compactMap { content -> String? in
      if case .text(let value) = content {
        return value
      }
      return nil
    }.joined()

    let audioData = completion.message.content.firstAudioData
    print("Final text: '\(text.prefix(200))'")
    print("Audio data: \(audioData?.count ?? 0) bytes")
    messages.append(
      AudioDemoMessage(
        role: .assistant,
        text: text.isEmpty ? "(audio response)" : text,
        audioData: audioData
      )
    )
    streamingText = ""
    isGenerating = false
    status =
      audioData != nil
      ? "Response complete with audio."
      : finishReasonDescription(completion.finishReason)

    // Audio already played via streaming (enqueue in handle(.audioSample))
    // Don't play again here to avoid duplicate playback
  }

  private func handleGenerationError(_ error: Error) {
    isGenerating = false
    streamingText = ""
    status = "Generation failed: \(error.localizedDescription)"
  }

  private func finishReasonDescription(_ reason: GenerationFinishReason) -> String {
    switch reason {
    case .stop:
      return "Response complete."
    case .exceed_context:
      return "Context window exceeded."
    }
  }

  private func findModelURL() -> URL? {
    let bundle = Bundle.main
    print("Bundle: \(bundle.bundlePath)")
    print("Bundle GGUF resources: \(bundle.urls(forResourcesWithExtension: "gguf", subdirectory: nil) ?? [])")
    
    let candidates = [
      "LFM2.5-Audio-1.5B-Q8_0"
    ]
    for name in candidates {
      if let url = bundle.url(forResource: name, withExtension: "gguf") {
        print("Found model at: \(url)")
        return url
      }
      if let url = bundle.url(forResource: name, withExtension: "gguf", subdirectory: "Resources") {
        print("Found model in Resources: \(url)")
        return url
      }
    }
    print("No model found in bundle")
    return nil
  }
}

extension Array where Element == ChatMessageContent {
  fileprivate var firstAudioData: Data? {
    for content in self {
      if case .audio(let data) = content {
        return data
      }
    }
    return nil
  }
}
